{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Evaluating Machine Learning Models\n",
    "\n",
    "Once we've trained a model, how do we know if it's any good? Accuracy is a common metric, but it can be very misleading, especially for imbalanced datasets.\n",
    "\n",
    "This notebook covers two of the most important concepts in model evaluation:\n",
    "\n",
    "1.  **Classification Metrics**: Precision, Recall, and F1-Score.\n",
    "2.  **Model Diagnostics**: The Bias-Variance Tradeoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d423033",
   "metadata": {},
   "source": [
    "What Are Precision, Recall, and F1 Score?\n",
    "\n",
    "Before jumping into the calculations and code, let’s define these terms:\n",
    "\n",
    "    Precision: Measures the accuracy of positive predictions. It answers the question, “Of all the items the model labeled as positive, how many were actually positive?”\n",
    "    Recall (Sensitivity): Measures the model’s ability to find all the positive instances. It answers the question, “Of all the actual positives, how many did the model correctly identify?”\n",
    "    F1 Score: The harmonic mean of precision and recall. It balances the two metrics into a single number, making it especially useful when precision and recall are in trade-off.\n",
    "\n",
    "Why Accuracy Isn’t Always Enough\n",
    "\n",
    "While accuracy is often the first metric to evaluate, it can be misleading in imbalanced datasets. For example:\n",
    "\n",
    "    Imagine a dataset where 99% of the data belongs to Class A and only 1% to Class B.\n",
    "    A model that always predicts Class A would have 99% accuracy but would completely fail to detect Class B."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precision-recall-intro",
   "metadata": {},
   "source": [
    "# 1. Precision, Recall, and F1-Score\n",
    "\n",
    "These metrics are used for **classification** tasks and are much more informative than simple accuracy.\n",
    "\n",
    "Imagine an email spam detector:\n",
    "- **True Positive (TP)**: A spam email is correctly identified as spam.\n",
    "- **True Negative (TN)**: A normal email is correctly identified as not spam.\n",
    "- **False Positive (FP)**: A normal email is incorrectly identified as spam (ouch!).\n",
    "- **False Negative (FN)**: A spam email is incorrectly identified as normal (annoying).\n",
    "\n",
    "These four outcomes are summarized in a **confusion matrix**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confusion-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's imagine some true labels and model predictions\n",
    "y_true = [0, 1, 0, 1, 0, 0, 1, 1, 0, 1] # 0: Not Spam, 1: Spam\n",
    "y_pred = [0, 1, 0, 0, 1, 0, 1, 1, 0, 1] # Model's predictions\n",
    "\n",
    "# Scikit-learn makes it easy to calculate the confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nTrue Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metrics-def",
   "metadata": {},
   "source": [
    "### Precision: The 'Purity' of Positive Predictions\n",
    "\n",
    "Measures the accuracy of positive predictions. It answers the question, “Of all the items the model labeled as positive, how many were actually positive?”\n",
    "<br/><br/>\n",
    "**Question**: Of all the emails the model flagged as spam, what fraction were *actually* spam?\n",
    "\n",
    "`Precision = TP / (TP + FP)`\n",
    "\n",
    "A high precision means the model is trustworthy when it says something is spam (it generates few false positives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "precision-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = precision_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recall-def",
   "metadata": {},
   "source": [
    "### Recall (Sensitivity): Finding All the Positives\n",
    "\n",
    "Measures the model’s ability to find all the positive instances. It answers the question, “Of all the actual positives, how many did the model correctly identify?”\n",
    "<br/><br/>\n",
    "**Question**: Of all the emails that were *actually* spam, what fraction did the model correctly identify?\n",
    "\n",
    "`Recall = TP / (TP + FN)`\n",
    "\n",
    "A high recall means the model is good at finding all the spam emails (it generates few false negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recall-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Recall: {recall:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1-def",
   "metadata": {},
   "source": [
    "### F1-Score: The Harmonic Mean\n",
    "\n",
    "The harmonic mean of precision and recall. It balances the two metrics into a single number, making it especially useful when precision and recall are in trade-off.\n",
    "<br/><br/>\n",
    "Often, there's a tradeoff between precision and recall. The F1-score provides a single metric that balances both.\n",
    "\n",
    "`F1 = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "\n",
    "It's the harmonic mean of precision and recall, and it gives a better measure of a model's performance than accuracy on imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = f1_score(y_true, y_pred)\n",
    "print(f\"F1-Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bias-variance-intro",
   "metadata": {},
   "source": [
    "# 2. The Bias-Variance Tradeoff\n",
    "\n",
    "This is one of the most fundamental concepts in machine learning. It helps us diagnose our model's errors.\n",
    "\n",
    "- **Bias**: The error from incorrect assumptions in the learning algorithm. High bias can cause the model to miss the relevant relations between features and target outputs (**underfitting**).\n",
    "- **Variance**: The error from sensitivity to small fluctuations in the training set. High variance can cause the model to model the random noise in the training data, rather than the intended outputs (**overfitting**).\n",
    "\n",
    "**The Goal**: Find a balance. A simple model has high bias and low variance. A complex model has low bias and high variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bias-variance-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a synthetic dataset with a non-linear relationship\n",
    "np.random.seed(42)\n",
    "x = np.linspace(-5, 5, 100)\n",
    "y_true = np.sin(x) + np.random.normal(0, 0.2, 100)\n",
    "\n",
    "# --- Model 1: Low Complexity (High Bias, Low Variance) ---\n",
    "# This is a simple linear model (degree 1 polynomial)\n",
    "p1 = np.polyfit(x, y_true, 1)\n",
    "y_pred1 = np.polyval(p1, x)\n",
    "\n",
    "# --- Model 2: High Complexity (Low Bias, High Variance) ---\n",
    "# This is a very complex model (degree 15 polynomial)\n",
    "p15 = np.polyfit(x, y_true, 15)\n",
    "y_pred15 = np.polyval(p15, x)\n",
    "\n",
    "# --- Model 3: 'Just Right' ---\n",
    "# This model's complexity is close to the true function\n",
    "p3 = np.polyfit(x, y_true, 3)\n",
    "y_pred3 = np.polyval(p3, x)\n",
    "\n",
    "# --- Visualization ---\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Underfitting\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.scatter(x, y_true, s=10, label='Data')\n",
    "plt.plot(x, y_pred1, color='red', label='Fit')\n",
    "plt.title('High Bias (Underfitting)')\n",
    "plt.legend()\n",
    "\n",
    "# Overfitting\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.scatter(x, y_true, s=10, label='Data')\n",
    "plt.plot(x, y_pred15, color='red', label='Fit')\n",
    "plt.title('High Variance (Overfitting)')\n",
    "plt.legend()\n",
    "\n",
    "# Good Fit\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(x, y_true, s=10, label='Data')\n",
    "plt.plot(x, y_pred3, color='red', label='Fit')\n",
    "plt.title('Good Balance')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-microprojects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
