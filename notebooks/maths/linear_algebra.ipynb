{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "206f2422",
   "metadata": {},
   "source": [
    "# Linear Algebra for Machine Learning\n",
    "\n",
    "Linear algebra is the language of data. We use it to represent and manipulate everything from user profiles and images to the weights of a neural network.\n",
    "\n",
    "This notebook builds geometric intuition for core concepts and connects them directly to modern ML applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2003752a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_sample_image\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be551cdc",
   "metadata": {},
   "source": [
    "# 1. Vectors: The Atoms of Data\n",
    "\n",
    "In ML, a vector is a list of numbers that represents a piece of data. Each number is a **feature**.\n",
    "\n",
    "**Example: A User Vector**\n",
    "Imagine we're building a recommendation system. A user could be represented by a vector:\n",
    "\n",
    "`user_a = [age, movies_watched, avg_rating]`\n",
    "`user_a = [34, 150, 4.5]`\n",
    "\n",
    "This vector is a single point in a 3-dimensional 'user space'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaa0e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A data point is a vector\n",
    "user_a = np.array([34, 150, 4.5])\n",
    "\n",
    "# Model weights are also vectors (or matrices)\n",
    "weights = np.array([0.1, -0.5, 2.0])\n",
    "\n",
    "print(\"User A:\", user_a)\n",
    "print(\"Model Weights:\", weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b726ce0f",
   "metadata": {},
   "source": [
    "# 2. The Dot Product: Measuring Similarity\n",
    "\n",
    "The dot product tells us how 'aligned' two vectors are.\n",
    "\n",
    "`v · w = ||v|| ||w|| cos(θ)`\n",
    "\n",
    "Its most important use in modern ML is to measure **similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c1cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_a = np.array([5, 4]) # Likes action and sci-fi\n",
    "user_b = np.array([4, 5]) # Likes action and sci-fi (similar)\n",
    "user_c = np.array([1, 2]) # Likes comedy and romance (dissimilar)\n",
    "\n",
    "similarity_ab = np.dot(user_a, user_b)\n",
    "similarity_ac = np.dot(user_a, user_c)\n",
    "\n",
    "print(f\"Similarity between User A and B: {similarity_ab}\")\n",
    "print(f\"Similarity between User A and C: {similarity_ac}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674394c",
   "metadata": {},
   "source": [
    "### Modern Application: Attention Mechanisms\n",
    "\n",
    "The core of the Transformer architecture (used in models like GPT) is the **scaled dot-product attention**.\n",
    "\n",
    "1. Every word in a sentence is represented by a vector (a 'Query' vector `Q`).\n",
    "2. To decide which other words to pay attention to, this `Q` vector is 'dotted' with the 'Key' vectors (`K`) of all other words.\n",
    "3. A large dot product score means high similarity, so the model pays more attention to that word when processing the current one.\n",
    "\n",
    "**`Attention_Score = dot(Q, K)`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e39b9c0",
   "metadata": {},
   "source": [
    "# 3. Matrix Multiplication: Transforming Data\n",
    "\n",
    "A matrix is a collection of vectors. In ML, a matrix multiplication `Wx` means applying a **linear transformation** to the data vector `x`.\n",
    "\n",
    "This is the fundamental operation of a neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5197e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A transformation matrix (e.g., a neural network layer's weights)\n",
    "W = np.array([\n",
    "    [0.8, 0.2],\n",
    "    [-0.3, 1.1]\n",
    "])\n",
    "\n",
    "# An input data vector\n",
    "x = np.array([2, 3])\n",
    "\n",
    "# The transformation\n",
    "output = W @ x\n",
    "\n",
    "print(\"Input vector:\", x)\n",
    "print(\"Transformed vector:\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcbc4a9",
   "metadata": {},
   "source": [
    "### The Power of Batching\n",
    "\n",
    "Why do we use GPUs? Because they are masters of matrix multiplication.\n",
    "\n",
    "Instead of processing one data vector `x` at a time, we can stack them into a data **matrix** `X` (a 'batch') and process them all in one go.\n",
    "\n",
    "`Output_Matrix = Data_Matrix @ Weight_Matrix`\n",
    "`(num_samples, num_neurons) = (num_samples, num_features) @ (num_features, num_neurons)`\n",
    "\n",
    "This is why linear algebra is the language of deep learning performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "batch-processing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A batch of 4 data samples, each with 2 features\n",
    "X_batch = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [-1, 2]\n",
    "])\n",
    "\n",
    "# A weight matrix for a layer with 3 neurons\n",
    "W_layer = np.random.rand(2, 3)\n",
    "\n",
    "# Process the entire batch at once\n",
    "batch_output = X_batch @ W_layer\n",
    "\n",
    "print(\"Input Batch Shape:\", X_batch.shape)\n",
    "print(\"Weight Matrix Shape:\", W_layer.shape)\n",
    "print(\"\nOutput Batch Shape:\", batch_output.shape)\n",
    "print(\"\nOutput Batch:\\n\", batch_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d6392",
   "metadata": {},
   "source": [
    "# 4. Singular Value Decomposition (SVD): Compressing Information\n",
    "\n",
    "SVD is a powerful factorization that breaks any matrix `A` into three other matrices:\n",
    "\n",
    "`A = U * S * Vᵀ`\n",
    "\n",
    "- `U` and `V` are orthogonal matrices (representing rotations).\n",
    "- `S` is a diagonal matrix of **singular values**.\n",
    "\n",
    "The singular values in `S` tell us the 'importance' of each dimension. By throwing away the least important ones, we can create a **low-rank approximation** of the original matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "svd-use-case",
   "metadata": {},
   "source": [
    "### Practical Use Case: Image Compression\n",
    "\n",
    "An image is just a matrix of pixel values. We can use SVD to compress it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802e2264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image and convert to grayscale\n",