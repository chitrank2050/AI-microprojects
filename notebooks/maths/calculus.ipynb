{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff5bb07",
   "metadata": {},
   "source": [
    "# Calculus for Machine Learning\n",
    "\n",
    "Calculus is the engine of optimization.\n",
    "\n",
    "Machine Learning =\n",
    "1. Define a loss function\n",
    "2. Compute derivatives\n",
    "3. Update parameters using gradients\n",
    "\n",
    "This notebook builds intuition for:\n",
    "- Derivatives\n",
    "- Partial derivatives\n",
    "- Gradients\n",
    "- Chain rule\n",
    "- Gradient Descent\n",
    "- Backpropagation intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4901492d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc4e4fc3",
   "metadata": {},
   "source": [
    "# 1. A Real-World Problem: Predicting House Prices\n",
    "\n",
    "Let's ground our calculus in a simple, concrete machine learning problem.\n",
    "\n",
    "**Goal:** Predict a house's price based on its size (square footage).\n",
    "\n",
    "Our model will be a simple linear one:\n",
    "\n",
    "`price = w * size`\n",
    "\n",
    "**Question:** How do we find the best value for the weight `w`?\n",
    "\n",
    "**Answer:** We define a **loss function** that measures how bad our model is. Then, we use calculus to find the `w` that minimizes this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-house-data",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some synthetic data\n",
    "np.random.seed(42)\n",
    "sizes = np.random.randn(50) + 1.5  # Avg size 1500 sq ft\n",
    "prices = (sizes * 3) + np.random.randn(50) * 0.5 # Price = 3x size + noise\n",
    "\n",
    "plt.scatter(sizes, prices)\n",
    "plt.xlabel(\"Size (1000s sq ft)\")\n",
    "plt.ylabel(\"Price ($100k)\")\n",
    "plt.title(\"Our Housing Data\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-loss-def",
   "metadata": {},
   "source": [
    "# 2. The Loss Function: Mean Squared Error (MSE)\n",
    "\n",
    "The MSE is a common loss function. It's simply the average of the squared differences between the predicted prices and the actual prices.\n",
    "\n",
    "`L(w) = (1/n) * Σ ( (w * size_i) - price_i )²`\n",
    "\n",
    "This is a function of `w`. Our goal is to find the `w` that makes `L(w)` as small as possible. This is an optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8414f72",
   "metadata": {},
   "source": [
    "# Visualizing the Loss Landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loss-landscape-plot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define our loss function in Python\n",
    "def loss_function(w, sizes, prices):\n",
    "    predictions = w * sizes\n",
    "    return np.mean((predictions - prices)**2)\n",
    "\n",
    "# Let's see how the loss changes for different values of w\n",
    "w_values = np.linspace(0, 6, 100)\n",
    "loss_values = [loss_function(w, sizes, prices) for w in w_values]\n",
    "\n",
    "plt.plot(w_values, loss_values)\n",
    "plt.xlabel(\"Weight 'w'\")\n",
    "plt.ylabel(\"Loss (MSE)\")\n",
    "plt.title(\"Loss Landscape: Find the Minimum!\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de1c1cb",
   "metadata": {},
   "source": [
    "# 3. Multivariable Functions & Gradients\n",
    "\n",
    "Our loss function for house prices only had one parameter `w`. In reality, models have millions.\n",
    "\n",
    "Let's imagine a model with two parameters, `w` and `b` (a bias term):\n",
    "\n",
    "`price = w * size + b`\n",
    "\n",
    "The loss `L(w, b)` is now a function of two variables. We need to compute **partial derivatives** to find the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77fd44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(w, b, sizes, prices):\n",
    "    predictions = w * sizes + b\n",
    "    return np.mean((predictions - prices)**2)\n",
    "\n",
    "# Let's visualize this 2D loss surface\n",
    "w_vals = np.linspace(0, 6, 50)\n",
    "b_vals = np.linspace(-3, 3, 50)\n",
    "W, B = np.meshgrid(w_vals, b_vals)\n",
    "L = np.zeros(W.shape)\n",
    "\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        L[i, j] = f2(W[i, j], B[i, j], sizes, prices)\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_surface(W, B, L, cmap='viridis', alpha=0.7)\n",
    "ax.set_xlabel('Weight (w)')\n",
    "ax.set_ylabel('Bias (b)')\n",
    "ax.set_zlabel('Loss')\n",
    "ax.set_title('Loss Surface for L(w, b)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7000192a",
   "metadata": {},
   "source": [
    "The **gradient** is a vector of these partial derivatives:\n",
    "\n",
    "`∇L(w,b) = [∂L/∂w, ∂L/∂b]`\n",
    "\n",
    "It always points in the direction of the steepest ascent on the loss surface. To minimize the loss, we must go in the **opposite** direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523c3eab",
   "metadata": {},
   "source": [
    "# 4. Chain Rule & Backpropagation\n",
    "\n",
    "The chain rule is the engine that lets us calculate the derivatives for complex, multi-layered functions like neural networks. It's the core idea behind **backpropagation**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d487f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    return x**2\n",
    "\n",
    "def f_outer(u):\n",
    "    return np.sin(u)\n",
    "\n",
    "def composite(x):\n",
    "    return f_outer(g(x))\n",
    "\n",
    "# The derivative dz/dx = f'(g(x)) * g'(x)\n",
    "x = 1.5\n",
    "analytical = np.cos(g(x)) * (2*x)\n",
    "print(\"Analytical derivative:\", analytical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "# 4a. Automatic Differentiation (The Modern Way)\n",
    "\n",
    "In practice, nobody calculates these derivatives by hand. Modern deep learning frameworks (like PyTorch and TensorFlow) do it for us.\n",
    "\n",
    "This is called **automatic differentiation**.\n",
    "\n",
    "The framework builds a **computational graph** and then uses the chain rule to automatically compute gradients for all parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define tensors (our parameters). requires_grad=True tells PyTorch to track them.\n",
    "w = torch.tensor(0.0, requires_grad=True)\n",
    "b = torch.tensor(0.0, requires_grad=True)\n",
    "\n",
    "# Convert our data to PyTorch tensors\n",
    "sizes_t = torch.from_numpy(sizes).float()\n",
    "prices_t = torch.from_numpy(prices).float()\n",
    "\n",
    "# Define our loss function\n",
    "predictions = w * sizes_t + b\n",
    "loss = torch.mean((predictions - prices_t)**2)\n",
    "\n",
    "# Backpropagate: PyTorch calculates all gradients\n",
    "loss.backward()\n",
    "\n",
    "# The gradients are now stored in the .grad attribute\n",
    "print(f\"Gradient ∂L/∂w (computed by PyTorch): {w.grad}\")\n",
    "print(f\"Gradient ∂L/∂b (computed by PyTorch): {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da183aad",
   "metadata": {},
   "source": [
    "# 5. Gradient Descent in Action\n",
    "\n",
    "Now we can put it all together to find the best `w` and `b` for our house price problem.\n",
    "\n",
    "The update rule is:\n",
    "\n",
    "`θ = θ - η * ∇L(θ)`\n",
    "\n",
    "Where `θ` is the vector of our parameters `[w, b]` and `η` (eta) is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc874e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize parameters\n",
    "w = 0.0\n",
    "b = 0.0\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.1\n",
    "epochs = 80\n",
    "\n",
    "history = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    # 1. Calculate predictions\n",
    "    predictions = w * sizes + b\n",
    "\n",
    "    # 2. Calculate the gradients of the loss function\n",
    "    # ∂L/∂w = (2/n) * Σ ( (w*x + b) - y ) * x\n",
    "    grad_w = 2 * np.mean((predictions - prices) * sizes)\n",
    "    # ∂L/∂b = (2/n) * Σ ( (w*x + b) - y )\n",
    "    grad_b = 2 * np.mean(predictions - prices)\n",
    "\n",
    "    # 3. Update parameters\n",
    "    w = w - learning_rate * grad_w\n",
    "    b = b - learning_rate * grad_b\n",
    "\n",
    "    # Log the loss\n",
    "    loss = np.mean((predictions - prices)**2)\n",
    "    history.append(loss)\n",
    "    if i % 10 == 0:\n",
    "      print(f'Epoch {i}, Loss: {loss:.4f}')\n",
    "\n",
    "print(f\"\\nFinal parameters: w = {w:.4f}, b = {b:.4f}\")\n",
    "print(f\"Final loss: {loss:.4f}\")\n",
    "\n",
    "plt.plot(history)\n",
    "plt.title('Loss Convergence')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
    {
   "cell_type": "markdown",
   "id": "final-plot",
   "metadata": {},
   "source": [
    "# 6. Visualizing the Result\n",
    "\n",
    "Let's plot our learned line against the original data."
   ]
  },
  {
      "cell_type": "code",
      "execution_count": null,
      "id": "final-plot-code",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.scatter(sizes, prices, label='Data')\n",
        "plt.plot(sizes, w * sizes + b, color='red', label='Our Fit')\n",
        "plt.xlabel(\"Size (1000s sq ft)\")\n",
        "plt.ylabel(\"Price ($100k)\")\n",
        "plt.title(\"Learned Model vs. Data\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()"
      ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0675f",
   "metadata": {},
   "source": [
    "# Key Takeaways\n",
    "\n",
    "- **Loss Function**: We need a way to measure how wrong our model is. Calculus helps us minimize this 'wrongness'.\n",
    "- **Gradient**: The gradient of the loss function tells us the direction to update our parameters to make the model better.\n",
    "- **Gradient Descent**: By repeatedly taking small steps in the opposite direction of the gradient, we can find the optimal parameters for our model.\n",
    "- **Backpropagation**: Is just the chain rule applied over and over in a neural network to calculate the gradient efficiently.\n",
    "- **Automatic Differentiation**: Modern frameworks do the heavy lifting of calculus for us, but understanding the principles is crucial for debugging and advanced modeling."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI-microprojects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
```